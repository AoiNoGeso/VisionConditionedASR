import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
import os
from dataclasses import dataclass
from typing import Optional, List, Dict
import numpy as np
from tqdm import tqdm
from pyctcdecode import build_ctcdecoder
import jiwer

from model import VisionConditionedASR
from dataloader import create_dataloader
from train import TrainingConfig


@dataclass
class TestConfig:
    """è©•ä¾¡è¨­å®š"""
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè¨­å®š
    checkpoint_path: str = "../checkpoints/checkpoint_epoch_4.pt"
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
    val_json: str = "../../Datasets/SpokenCOCO/SpokenCOCO_val_fixed.json"
    audio_dir: str = "../../Datasets/SpokenCOCO"
    image_dir: str = "../../Datasets/stair_captions/images"
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰è‡ªå‹•å–å¾—ã•ã‚Œã‚‹ãŒã€å¿µã®ãŸã‚ï¼‰
    vocab_size: Optional[int] = None
    hidden_dim: int = 256
    num_heads: int = 2
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼è¨­å®š
    batch_size: int = 16
    num_workers: int = 4
    max_audio_length: float = 10.0
    validate_files: bool = True
    
    # ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¨­å®š
    use_beam_search: bool = True
    beam_width: int = 10
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device: str = "cuda:1"
    
    # çµæœä¿å­˜è¨­å®š
    save_results: bool = True
    results_dir: str = "../results"


class CTCDecoder:
    """pyctcdecodeã‚’ä½¿ç”¨ã—ãŸCTCãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self, tokenizer, use_beam_search: bool = True, beam_width: int = 100):
        """
        Args:
            tokenizer: Hugging Face tokenizer
            use_beam_search: ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã‚’ä½¿ç”¨ã™ã‚‹ã‹
            beam_width: ãƒ“ãƒ¼ãƒ ã®å¹…
        """
        self.tokenizer = tokenizer
        self.use_beam_search = use_beam_search
        self.beam_width = beam_width
        
        # èªå½™ãƒªã‚¹ãƒˆã®ä½œæˆ
        vocab_list = []
        for i in range(tokenizer.vocab_size):
            token = tokenizer.convert_ids_to_tokens(i)
            # ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ–‡å­—åˆ—ã§ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ã«å¤‰æ›
            if token is None:
                token = ""
            vocab_list.append(token)
        
        # pyctcdecodeã®ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’æ§‹ç¯‰ï¼ˆè¨€èªãƒ¢ãƒ‡ãƒ«ãªã—ï¼‰
        self.decoder = build_ctcdecoder(
            labels=vocab_list,
            kenlm_model_path=None  # è¨€èªãƒ¢ãƒ‡ãƒ«ãªã—
        )
        
        print(f"[CTCDecoder] Initialized")
        print(f"  Vocabulary size: {len(vocab_list)}")
        print(f"  Beam search: {use_beam_search}")
        if use_beam_search:
            print(f"  Beam width: {beam_width}")
    
    def decode(self, logits: torch.Tensor) -> List[str]:
        """
        ãƒ­ã‚°itsé…åˆ—ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«ãƒ‡ã‚³ãƒ¼ãƒ‰
        
        Args:
            logits: [batch_size, seq_len, vocab_size]
        
        Returns:
            ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        batch_size = logits.size(0)
        results = []
        
        # ãƒãƒƒãƒå†…ã®å„ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        for i in range(batch_size):
            logits_i = logits[i].cpu().numpy()  # [seq_len, vocab_size]
            
            if self.use_beam_search:
                # ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                text = self.decoder.decode(
                    logits_i,
                    beam_width=self.beam_width
                )
            else:
                # è²ªæ¬²æ³•ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                text = self.decoder.decode(logits_i, beam_width=1)
            
            results.append(text)
        
        return results


def compute_wer(references: List[str], hypotheses: List[str]) -> Dict[str, float]:
    """
    WERï¼ˆWord Error Rateï¼‰ã‚’è¨ˆç®—
    
    Args:
        references: æ­£è§£ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        hypotheses: äºˆæ¸¬ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
    
    Returns:
        WERçµ±è¨ˆæƒ…å ±ã®è¾æ›¸
    """
    # ğŸ’¡ä¿®æ­£: jiwerã®æ–°ã—ã„APIã‚’ä½¿ç”¨
    output = jiwer.process_words(references, hypotheses)
    
    # ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‹ã‚‰çµ±è¨ˆã‚’é›†è¨ˆ
    total_substitutions = 0
    total_deletions = 0
    total_insertions = 0
    total_hits = 0
    
    for alignment in output.alignments:
        for op in alignment:
            if op.type == 'substitute':
                total_substitutions += 1
            elif op.type == 'delete':
                total_deletions += 1
            elif op.type == 'insert':
                total_insertions += 1
            elif op.type == 'equal':
                total_hits += 1
    
    return {
        'wer': output.wer * 100,  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã«å¤‰æ›
        'mer': output.mer * 100,
        'wil': output.wil * 100,
        'substitutions': total_substitutions,
        'deletions': total_deletions,
        'insertions': total_insertions,
        'hits': total_hits
    }


def load_checkpoint(checkpoint_path: str, model: VisionConditionedASR, device: torch.device):
    """
    ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        checkpoint_path: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        model: ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        device: ãƒ‡ãƒã‚¤ã‚¹
    
    Returns:
        epoch: å­¦ç¿’æ¸ˆã¿ã‚¨ãƒãƒƒã‚¯æ•°
    """
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    print(f"\n[Loading] Loading checkpoint from: {checkpoint_path}")
    # ğŸ’¡ä¿®æ­£: weights_only=Falseã‚’æŒ‡å®šï¼ˆä¿¡é ¼ã§ãã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å ´åˆï¼‰
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæƒ…å ±ã‚’è¡¨ç¤º
    epoch = checkpoint.get('epoch', -1)
    train_loss = checkpoint.get('train_loss', 0.0)
    val_loss = checkpoint.get('val_loss', 0.0)
    
    print(f"[Loading] Checkpoint loaded successfully")
    print(f"  Epoch: {epoch + 1}")
    print(f"  Train Loss: {train_loss:.4f}")
    print(f"  Val Loss: {val_loss:.4f}")
    
    return epoch + 1


def evaluate(
    model: VisionConditionedASR,
    dataloader: DataLoader,
    decoder: CTCDecoder,
    device: torch.device,
    config: TestConfig
):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¦WERã‚’è¨ˆç®—
    
    Args:
        model: è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        dataloader: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        decoder: CTCãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼
        device: ãƒ‡ãƒã‚¤ã‚¹
        config: è©•ä¾¡è¨­å®š
    
    Returns:
        results: è©•ä¾¡çµæœã®è¾æ›¸
    """
    model.eval()
    
    all_references = []
    all_hypotheses = []
    all_samples = []  # è©³ç´°ãªçµæœã‚’ä¿å­˜
    
    print(f"\n{'='*60}")
    print("Starting Evaluation")
    print(f"{'='*60}")
    print(f"Dataset size: {len(dataloader.dataset)}")
    print(f"Number of batches: {len(dataloader)}")
    print(f"{'='*60}\n")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="Evaluating")):
            try:
                # Forward pass
                logits = model(batch)  # [B, T, vocab_size]
                
                # ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                hypotheses = decoder.decode(logits)
                references = batch["text"]
                
                # çµæœã‚’ä¿å­˜
                all_references.extend(references)
                all_hypotheses.extend(hypotheses)
                
                # è©³ç´°ãªçµæœã‚’ä¿å­˜ï¼ˆæœ€åˆã®100ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ï¼‰
                if len(all_samples) < 100:
                    for ref, hyp in zip(references, hypotheses):
                        all_samples.append({
                            'reference': ref,
                            'hypothesis': hyp
                        })
                
            except Exception as e:
                print(f"\n[Error] Exception at batch {batch_idx}: {e}")
                import traceback
                traceback.print_exc()
                continue
    
    # WERã®è¨ˆç®—
    print(f"\n{'='*60}")
    print("Computing WER...")
    print(f"{'='*60}")
    
    wer_metrics = compute_wer(all_references, all_hypotheses)
    
    # çµæœã®è¡¨ç¤º
    print(f"\n{'='*60}")
    print("Evaluation Results")
    print(f"{'='*60}")
    print(f"Total samples: {len(all_references)}")
    print(f"\nWord Error Rate (WER): {wer_metrics['wer']:.2f}%")
    print(f"Match Error Rate (MER): {wer_metrics['mer']:.2f}%")
    print(f"Word Information Lost (WIL): {wer_metrics['wil']:.2f}%")
    print(f"\nError Breakdown:")
    print(f"  Substitutions: {wer_metrics['substitutions']}")
    print(f"  Deletions:     {wer_metrics['deletions']}")
    print(f"  Insertions:    {wer_metrics['insertions']}")
    print(f"  Hits:          {wer_metrics['hits']}")
    print(f"{'='*60}\n")
    
    # ã‚µãƒ³ãƒ—ãƒ«çµæœã®è¡¨ç¤º
    print(f"{'='*60}")
    print("Sample Predictions (first 5):")
    print(f"{'='*60}")
    for i, sample in enumerate(all_samples[:5]):
        print(f"\nSample {i+1}:")
        print(f"  Reference:  {sample['reference']}")
        print(f"  Hypothesis: {sample['hypothesis']}")
    print(f"{'='*60}\n")
    
    return {
        'wer_metrics': wer_metrics,
        'num_samples': len(all_references),
        'references': all_references,
        'hypotheses': all_hypotheses,
        'samples': all_samples
    }


def save_results(results: Dict, config: TestConfig, checkpoint_epoch: int):
    """
    è©•ä¾¡çµæœã‚’ä¿å­˜
    
    Args:
        results: è©•ä¾¡çµæœ
        config: è©•ä¾¡è¨­å®š
        checkpoint_epoch: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚¨ãƒãƒƒã‚¯æ•°
    """
    os.makedirs(config.results_dir, exist_ok=True)
    
    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    results_file = os.path.join(
        config.results_dir,
        f"wer_results_epoch_{checkpoint_epoch}.txt"
    )
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(results_file, 'w', encoding='utf-8') as f:
        f.write("="*60 + "\n")
        f.write("WER Evaluation Results\n")
        f.write("="*60 + "\n\n")
        
        f.write(f"Checkpoint: {config.checkpoint_path}\n")
        f.write(f"Epoch: {checkpoint_epoch}\n")
        f.write(f"Dataset: {config.val_json}\n")
        f.write(f"Beam Search: {config.use_beam_search}\n")
        if config.use_beam_search:
            f.write(f"Beam Width: {config.beam_width}\n")
        f.write(f"\nTotal samples: {results['num_samples']}\n")
        f.write("\n" + "="*60 + "\n")
        f.write("Metrics:\n")
        f.write("="*60 + "\n")
        f.write(f"WER: {results['wer_metrics']['wer']:.2f}%\n")
        f.write(f"MER: {results['wer_metrics']['mer']:.2f}%\n")
        f.write(f"WIL: {results['wer_metrics']['wil']:.2f}%\n")
        f.write(f"\nError Breakdown:\n")
        f.write(f"  Substitutions: {results['wer_metrics']['substitutions']}\n")
        f.write(f"  Deletions:     {results['wer_metrics']['deletions']}\n")
        f.write(f"  Insertions:    {results['wer_metrics']['insertions']}\n")
        f.write(f"  Hits:          {results['wer_metrics']['hits']}\n")
        f.write("\n" + "="*60 + "\n")
        f.write("Sample Predictions (first 20):\n")
        f.write("="*60 + "\n\n")
        
        for i, sample in enumerate(results['samples'][:20]):
            f.write(f"Sample {i+1}:\n")
            f.write(f"  REF: {sample['reference']}\n")
            f.write(f"  HYP: {sample['hypothesis']}\n\n")
    
    print(f"[Results] Saved to: {results_file}\n")
    
    # è©³ç´°ãªçµæœã‚’CSVã§ä¿å­˜
    csv_file = os.path.join(
        config.results_dir,
        f"predictions_epoch_{checkpoint_epoch}.csv"
    )
    
    import csv
    with open(csv_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Index', 'Reference', 'Hypothesis'])
        for i, (ref, hyp) in enumerate(zip(results['references'], results['hypotheses'])):
            writer.writerow([i+1, ref, hyp])
    
    print(f"[Results] Predictions saved to: {csv_file}\n")


def main():
    """ãƒ¡ã‚¤ãƒ³è©•ä¾¡é–¢æ•°"""
    # è¨­å®šã®åˆæœŸåŒ–
    config = TestConfig()
    
    # ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
    device = torch.device(config.device if torch.cuda.is_available() else "cpu")
    print(f"\n{'='*60}")
    print("Test Configuration")
    print(f"{'='*60}")
    print(f"Device: {device}")
    print(f"Checkpoint: {config.checkpoint_path}")
    print(f"Batch size: {config.batch_size}")
    print(f"Beam search: {config.use_beam_search}")
    if config.use_beam_search:
        print(f"Beam width: {config.beam_width}")
    print(f"{'='*60}\n")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
    print("[Setup] Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained("facebook/wav2vec2-base-960h")
    
    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    print("[Setup] Initializing model...")
    model = VisionConditionedASR(
        vocab_size=config.vocab_size,
        hidden_dim=config.hidden_dim,
        num_heads=config.num_heads,
        device=device
    ).to(device)
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰
    checkpoint_epoch = load_checkpoint(config.checkpoint_path, model, device)
    
    # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®åˆæœŸåŒ–
    print("\n[Setup] Initializing decoder...")
    decoder = CTCDecoder(
        tokenizer=tokenizer,
        use_beam_search=config.use_beam_search,
        beam_width=config.beam_width
    )
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ
    print("\n[Setup] Creating dataloader...")
    val_loader = create_dataloader(
        json_path=config.val_json,
        audio_dir=config.audio_dir,
        image_dir=config.image_dir,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        max_audio_length=config.max_audio_length,
        validate_files=config.validate_files
    )
    
    # è©•ä¾¡ã®å®Ÿè¡Œ
    results = evaluate(
        model=model,
        dataloader=val_loader,
        decoder=decoder,
        device=device,
        config=config
    )
    
    # çµæœã®ä¿å­˜
    if config.save_results:
        save_results(results, config, checkpoint_epoch)
    
    print("="*60)
    print("Evaluation Completed!")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()