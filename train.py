import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCTC
from dataclasses import dataclass
import wandb
import os
import random
import numpy as np

from dataloader import SpokenCOCODataset, spokenCOCO_collate
from model import VisionConditionedASR

# ========================================
# è¨­å®šã‚¯ãƒ©ã‚¹
# ========================================
@dataclass
class TrainingConfig:
    """å­¦ç¿’ã«é–¢ã™ã‚‹è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"""
    # ã‚·ãƒ¼ãƒ‰å€¤
    seed: int = 42
    
    # wandbè¨­å®š
    use_wandb: bool = False  # wandbãƒ­ã‚®ãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
    wandb_project: str = "VisionConditionedASR"  # wandbãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå
    
    # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    epochs: int = 10
    batch_size: int = 8
    gradient_accumulation_steps: int = 16
    
    # å­¦ç¿’ç‡è¨­å®šï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ï¼‰
    learning_rate_audio_encoder: float = 1e-5   # Audio Encoder (pre trained)
    learning_rate_vision_encoder: float = 1e-5  # Vision Encoder (pre trained)
    learning_rate_cross_attention: float = 1e-5 # Cross Attention (new)
    learning_rate_classifier: float = 1e-5      # Classifier (new)
    
    # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
    warmup_steps: int = 500                     # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°
    use_scheduler: bool = True                  # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    
    # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
    max_grad_norm: float = 1.0                  # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®é–¾å€¤
    
    # å±¤ã®å‡çµè¨­å®š
    freeze_audio_encoder: bool = True         # Audio Encoderã‚’å‡çµ
    freeze_vision_encoder: bool = True         # Vision Encoderã‚’å‡çµ
    freeze_cross_attention: bool = False        # Cross Attentionã‚’å‡çµ
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹ï¼ˆå­¦ç¿’ç”¨ï¼‰
    audio_dir_train: str = '../../Datasets/SpokenCOCO/'
    image_dir_train: str = '../../Datasets/stair_captions/images/'
    train_json_path: str = '../../Datasets/SpokenCOCO/SpokenCOCO_train_fixed.json'
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹ï¼ˆè©•ä¾¡ç”¨ï¼‰
    audio_dir_val: str = '../../Datasets/SpokenCOCO/'
    image_dir_val: str = '../../Datasets/stair_captions/images/'
    val_json_path: str = '../../Datasets/SpokenCOCO/SpokenCOCO_val_fixed.json'
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ
    model_save_dir: str = '../model/'


# ========================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ========================================
def move_data_to_device(data, device):
    """
    ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’GPU/CPUã«è»¢é€
    
    Args:
        data: ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ï¼ˆè¾æ›¸å½¢å¼ï¼‰
        device: è»¢é€å…ˆãƒ‡ãƒã‚¤ã‚¹
        
    Returns:
        ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
    """
    result = {}
    for key, value in data.items():
        if isinstance(value, torch.Tensor):
            result[key] = value.to(device)
        else:
            result[key] = value
    return result


def calculate_wer(reference, hypothesis):
    """
    WER (Word Error Rate) ã‚’è¨ˆç®—
    
    å˜èªãƒ¬ãƒ™ãƒ«ã§ã®ç·¨é›†è·é›¢ã‚’ç”¨ã„ã¦ã€éŸ³å£°èªè­˜ã®ç²¾åº¦ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
    å€¤ãŒå°ã•ã„ã»ã©ç²¾åº¦ãŒé«˜ã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
    
    Args:
        reference: æ­£è§£ãƒ†ã‚­ã‚¹ãƒˆ
        hypothesis: äºˆæ¸¬ãƒ†ã‚­ã‚¹ãƒˆ
    
    Returns:
        WERå€¤ï¼ˆ0.0ã€œ1.0ä»¥ä¸Šï¼‰
    """
    ref_words = reference.lower().split()
    hyp_words = hypothesis.lower().split()
    
    # ç·¨é›†è·é›¢è¡Œåˆ—ã‚’åˆæœŸåŒ–
    n_ref = len(ref_words)
    n_hyp = len(hyp_words)
    dist_matrix = [[0] * (n_hyp + 1) for _ in range(n_ref + 1)]
    
    # å¢ƒç•Œæ¡ä»¶
    for i in range(n_ref + 1):
        dist_matrix[i][0] = i
    for j in range(n_hyp + 1):
        dist_matrix[0][j] = j
    
    # å‹•çš„è¨ˆç”»æ³•ã§ç·¨é›†è·é›¢ã‚’è¨ˆç®—
    for i in range(1, n_ref + 1):
        for j in range(1, n_hyp + 1):
            if ref_words[i-1] == hyp_words[j-1]:
                cost = 0
            else:
                cost = 1
            
            dist_matrix[i][j] = min(
                dist_matrix[i-1][j] + 1,      # å‰Šé™¤
                dist_matrix[i][j-1] + 1,      # æŒ¿å…¥
                dist_matrix[i-1][j-1] + cost  # ç½®æ›
            )
    
    # WERã‚’è¨ˆç®—ï¼ˆç©ºæ–‡å­—åˆ—ã®å ´åˆã®å‡¦ç†ã‚’å«ã‚€ï¼‰
    if n_ref == 0:
        return 0 if n_hyp == 0 else 1
    
    return dist_matrix[n_ref][n_hyp] / n_ref


def set_seed(seed: int):
    """
    å†ç¾æ€§ã®ãŸã‚ã«ã‚·ãƒ¼ãƒ‰å€¤ã‚’å›ºå®š
    
    Args:
        seed: å›ºå®šã™ã‚‹ã‚·ãƒ¼ãƒ‰å€¤
    """
    print(f"\n[Seed] Setting random seed to {seed}")
    
    # Pythonæ¨™æº–ã®randomãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
    random.seed(seed)
    
    # NumPy
    np.random.seed(seed)
    
    # PyTorch
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # ãƒãƒ«ãƒGPUå¯¾å¿œ
    
    # CuDNNã®æ±ºå®šçš„å‹•ä½œã‚’æœ‰åŠ¹åŒ–ï¼ˆè‹¥å¹²é€Ÿåº¦ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼‰
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    print("  âœ“ Random seed has been set for reproducibility")
    print("  âœ“ CuDNN deterministic mode enabled")


def decode_predictions(predicted_ids, tokenizer, pad_token_id):
    """
    CTCã®äºˆæ¸¬çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆã«ãƒ‡ã‚³ãƒ¼ãƒ‰
    
    Args:
        predicted_ids: äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ID [batch, seq_len]
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        pad_token_id: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®ID
        
    Returns:
        ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    decoded_texts = []
    
    for pred_ids_seq in predicted_ids:
        # CTCãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼šé€£ç¶šã™ã‚‹åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»
        pred_tokens = []
        prev_token = None
        
        for token_id in pred_ids_seq.tolist():
            if token_id != pad_token_id and token_id != prev_token:
                pred_tokens.append(token_id)
            prev_token = token_id
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
        decoded_text = tokenizer.decode(pred_tokens, skip_special_tokens=True)
        decoded_texts.append(decoded_text)
    
    return decoded_texts


# ========================================
# ãƒ¢ãƒ‡ãƒ«è¨­å®šé–¢æ•°
# ========================================
def freeze_layers(model, config):
    """
    æŒ‡å®šã•ã‚ŒãŸå±¤ã‚’å‡çµ
    
    Args:
        model: VisionConditionedASRãƒ¢ãƒ‡ãƒ«
        config: å­¦ç¿’è¨­å®š
    """
    print("\n[Freeze] Layer freeze configuration:")
    
    # Audio Encoderã®å‡çµ
    if config.freeze_audio_encoder:
        for param in model.audio_encoder.parameters():
            param.requires_grad = False
        print("  âœ“ Audio Encoder: FROZEN")
    else:
        print("  âœ“ Audio Encoder: Trainable")
    
    # Vision Encoderã®å‡çµ
    if config.freeze_vision_encoder:
        for param in model.vision_encoder.parameters():
            param.requires_grad = False
        print("  âœ“ Vision Encoder: FROZEN")
    else:
        print("  âœ“ Vision Encoder: Trainable")
    
    # Cross Attentionã®å‡çµ
    if config.freeze_cross_attention:
        for param in model.cross_attention.parameters():
            param.requires_grad = False
        print("  âœ“ Cross Attention: FROZEN")
    else:
        print("  âœ“ Cross Attention: Trainable")
    
    # Classifierã¯å¸¸ã«å­¦ç¿’å¯èƒ½
    print("  âœ“ Classifier: Trainable (always)")


def get_optimizer(model, config):
    """
    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ç•°ãªã‚‹å­¦ç¿’ç‡ã‚’è¨­å®šã—ãŸã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
    
    Args:
        model: VisionConditionedASRãƒ¢ãƒ‡ãƒ«
        config: å­¦ç¿’è¨­å®š
        
    Returns:
        ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
    """
    param_groups = []
    
    # Audio Encoder
    if not config.freeze_audio_encoder:
        param_groups.append({
            'params': model.audio_encoder.parameters(),
            'lr': config.learning_rate_audio_encoder,
            'name': 'audio_encoder'
        })
    
    # Vision Encoder
    if not config.freeze_vision_encoder:
        param_groups.append({
            'params': model.vision_encoder.parameters(),
            'lr': config.learning_rate_vision_encoder,
            'name': 'vision_encoder'
        })
    
    # Cross Attention
    if not config.freeze_cross_attention:
        param_groups.append({
            'params': model.cross_attention.parameters(),
            'lr': config.learning_rate_cross_attention,
            'name': 'cross_attention'
        })
    
    # Classifier
    param_groups.append({
        'params': model.classifier.parameters(),
        'lr': config.learning_rate_classifier,
        'name': 'classifier'
    })
    
    optimizer = optim.AdamW(param_groups)
    
    # å­¦ç¿’ç‡ã®æƒ…å ±ã‚’è¡¨ç¤º
    print("\n[Optimizer] Learning rate configuration:")
    for group in param_groups:
        print(f"  âœ“ {group['name']}: lr={group['lr']:.2e}")
    
    return optimizer


def get_scheduler(optimizer, config, total_steps):
    """
    ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ— + ã‚³ã‚µã‚¤ãƒ³ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’ä½œæˆ
    
    Args:
        optimizer: ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        config: å­¦ç¿’è¨­å®š
        total_steps: ç·å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°
        
    Returns:
        ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ï¼ˆä½¿ç”¨ã—ãªã„å ´åˆã¯Noneï¼‰
    """
    if not config.use_scheduler:
        print("\n[Scheduler] No scheduler will be used")
        return None
    
    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
    warmup_scheduler = LinearLR(
        optimizer,
        start_factor=0.01,
        end_factor=1.0,
        total_iters=config.warmup_steps
    )
    
    # ã‚³ã‚µã‚¤ãƒ³ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
    cosine_scheduler = CosineAnnealingLR(
        optimizer,
        T_max=total_steps - config.warmup_steps,
        eta_min=1e-7
    )
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®çµ„ã¿åˆã‚ã›
    scheduler = SequentialLR(
        optimizer,
        schedulers=[warmup_scheduler, cosine_scheduler],
        milestones=[config.warmup_steps]
    )
    
    print("\n[Scheduler] Learning rate scheduler:")
    print(f"  âœ“ Warmup steps: {config.warmup_steps}")
    print(f"  âœ“ Total steps: {total_steps}")
    print(f"  âœ“ Type: Linear warmup + Cosine annealing")
    
    return scheduler


# ========================================
# å­¦ç¿’é–¢æ•°
# ========================================
def train_one_epoch(model, dataloader, optimizer, scheduler, ctc_loss, 
                   tokenizer, config, device, epoch, global_step):
    """
    1ã‚¨ãƒãƒƒã‚¯åˆ†ã®å­¦ç¿’ã‚’å®Ÿè¡Œ
    
    Args:
        model: å­¦ç¿’å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«
        dataloader: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        optimizer: ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        scheduler: å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ï¼ˆNoneã®å ´åˆã‚‚ã‚ã‚Šï¼‰
        ctc_loss: CTCæå¤±é–¢æ•°
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        config: å­¦ç¿’è¨­å®š
        device: å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹
        epoch: ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯ç•ªå·
        global_step: ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ãƒ†ãƒƒãƒ—æ•°
        
    Returns:
        (å¹³å‡æå¤±, æ›´æ–°ã•ã‚ŒãŸglobal_step)
    """
    model.train()
    total_loss = 0
    optimizer.zero_grad()
    
    pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{config.epochs} (Train)")
    
    for batch_idx, batch in enumerate(pbar):
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
            data = move_data_to_device(batch, device)
            
            # ãƒ©ãƒ™ãƒ«ã®æº–å‚™
            labels = tokenizer(
                data["text"], 
                return_tensors="pt", 
                padding=True
            ).input_ids.to(device)
            label_lengths = torch.sum(labels != tokenizer.pad_token_id, dim=1)
            
            # é †ä¼æ’­
            logits = model(data)
            
            # NaN or Inf check
            if torch.isnan(logits).any() or torch.isinf(logits).any():
                print(f"\nğŸš¨ğŸš¨ CRITICAL ERROR: Logits contain NaN or Inf!")
                print(f"  Max value: {logits.abs().max()}")
                print(f"  Min value: {logits.abs().min()}")
                raise RuntimeError("Model output (logits) is numerically unstable.")
            
            # CTCæå¤±ã®è¨ˆç®—
            log_probs = torch.log_softmax(logits, dim=2).transpose(0, 1)
            input_lengths = torch.full(
                (logits.size(0),), 
                logits.size(1), 
                dtype=torch.long
            ).to(device)
            
            loss = ctc_loss(log_probs, labels, input_lengths, label_lengths)
            
            # å‹¾é…ç´¯ç©ã®ãŸã‚ã®æ­£è¦åŒ–
            loss = loss / config.gradient_accumulation_steps
            
            # é€†ä¼æ’­
            loss.backward()
            
            # æå¤±ã®è¨˜éŒ²
            total_loss += loss.item() * config.gradient_accumulation_steps
            
            # é€²æ—ãƒãƒ¼ã®æ›´æ–°ï¼ˆç¾åœ¨ã®å­¦ç¿’ç‡ã‚‚è¡¨ç¤ºï¼‰
            current_lr = optimizer.param_groups[0]['lr']
            pbar.set_postfix(
                loss=loss.item() * config.gradient_accumulation_steps,
                lr=f"{current_lr:.2e}"
            )
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ï¼ˆå‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ï¼‰
            if (batch_idx + 1) % config.gradient_accumulation_steps == 0:
                # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                torch.nn.utils.clip_grad_norm_(
                    model.parameters(), 
                    max_norm=config.max_grad_norm
                )
                
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
                optimizer.step()
                optimizer.zero_grad()
                
                # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®æ›´æ–°
                if scheduler is not None:
                    scheduler.step()
                
                # wandbã¸ã®ãƒ­ã‚®ãƒ³ã‚°
                if config.use_wandb and (global_step + 1) % 100 == 0:
                    log_dict = {
                        "train/batch_loss": loss.item() * config.gradient_accumulation_steps,
                        "train/learning_rate": current_lr
                    }
                    wandb.log(log_dict, step=global_step)
                
                global_step += 1
            
            # ãƒ¡ãƒ¢ãƒªç®¡ç†
            if batch_idx % 50 == 0 and device.type == 'cuda':
                torch.cuda.empty_cache()
                
        except Exception as e:
            print(f"\n[Error] Batch {batch_idx} failed: {e}")
            print(f"  Batch keys: {batch.keys()}")
            if "wav" in batch:
                print(f"  wav type: {type(batch['wav'])}")
            if "image" in batch:
                print(f"  image count: {len(batch['image'])}")
            raise e
    
    # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®æ®‹ã‚Šå‹¾é…ã‚’é©ç”¨
    if len(dataloader) % config.gradient_accumulation_steps != 0:
        torch.nn.utils.clip_grad_norm_(
            model.parameters(), 
            max_norm=config.max_grad_norm
        )
        optimizer.step()
        optimizer.zero_grad()
    
    avg_loss = total_loss / len(dataloader)
    return avg_loss, global_step


def validate(model, dataloader, ctc_loss, tokenizer, device):
    """
    æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡
    
    Args:
        model: è©•ä¾¡å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«
        dataloader: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        ctc_loss: CTCæå¤±é–¢æ•°
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        device: å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹
        
    Returns:
        (å¹³å‡æå¤±, å¹³å‡WER)
    """
    model.eval()
    total_loss = 0
    total_wer = 0
    
    print("\n[Validation] Starting...")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="Validation")):
            try:
                # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
                data = move_data_to_device(batch, device)
                
                # ãƒ©ãƒ™ãƒ«ã®æº–å‚™
                labels = tokenizer(
                    data["text"], 
                    return_tensors="pt", 
                    padding=True
                ).input_ids.to(device)
                label_lengths = torch.sum(labels != tokenizer.pad_token_id, dim=1)
                
                # é †ä¼æ’­
                logits = model(data)
                
                # æå¤±ã®è¨ˆç®—
                log_probs = torch.log_softmax(logits, dim=2).transpose(0, 1)
                input_lengths = torch.full(
                    (logits.size(0),), 
                    logits.size(1), 
                    dtype=torch.long
                ).to(device)
                
                loss = ctc_loss(log_probs, labels, input_lengths, label_lengths)
                total_loss += loss.item()
                
                # WERã®è¨ˆç®—
                predicted_ids = torch.argmax(logits, dim=-1)
                pred_texts = decode_predictions(
                    predicted_ids, 
                    tokenizer, 
                    tokenizer.pad_token_id
                )
                
                for pred_text, ref_text in zip(pred_texts, data["text"]):
                    total_wer += calculate_wer(ref_text, pred_text)
                
                # ãƒ¡ãƒ¢ãƒªç®¡ç†
                if batch_idx % 50 == 0 and device.type == 'cuda':
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                print(f"\n[Error] Validation batch {batch_idx} failed: {e}")
                continue
    
    avg_loss = total_loss / len(dataloader)
    avg_wer = total_wer / len(dataloader.dataset)
    
    return avg_loss, avg_wer


def save_model(model, epoch, config):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    
    Args:
        model: ä¿å­˜ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        epoch: ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯ç•ªå·
        config: å­¦ç¿’è¨­å®š
    """
    save_path = os.path.join(
        config.model_save_dir, 
        f'vision_conditioned_asr_epoch_{epoch+1}.pth'
    )
    torch.save(model.state_dict(), save_path)
    print(f"[Save] Model saved to {save_path}")


# ========================================
# ãƒ¡ã‚¤ãƒ³é–¢æ•°
# ========================================
def main():
    """ãƒ¡ã‚¤ãƒ³å­¦ç¿’ãƒ«ãƒ¼ãƒ—"""
    # è¨­å®šã®èª­ã¿è¾¼ã¿
    config = TrainingConfig()
    
    # ã‚·ãƒ¼ãƒ‰å€¤ã®è¨­å®š
    set_seed(config.seed)
    
    # ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
    device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
    print(f"[Setup] Using device: {device}")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    os.makedirs(config.model_save_dir, exist_ok=True)
    
    # wandbã®åˆæœŸåŒ–
    if config.use_wandb:
        wandb.init(project=config.wandb_project, config=config)
        print(f"[Wandb] Logging enabled - Project: {config.wandb_project}")
    else:
        print("[Wandb] Logging disabled")
    
    # ========================================
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
    # ========================================
    print("\n[Dataset] Loading training data...")
    train_dataset = SpokenCOCODataset(
        json_path=config.train_json_path,
        audio_dir=config.audio_dir_train,
        image_dir=config.image_dir_train
    )
    
    print("\n[Dataset] Loading validation data...")
    val_dataset = SpokenCOCODataset(
        json_path=config.val_json_path,
        audio_dir=config.audio_dir_val,
        image_dir=config.image_dir_val
    )
    
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=spokenCOCO_collate
    )
    
    val_dataloader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=spokenCOCO_collate
    )
    
    # ========================================
    # ãƒ¢ãƒ‡ãƒ«ãƒ»æœ€é©åŒ–ã®æº–å‚™
    # ========================================
    print("\n[Model] Initializing model and optimizer...")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨èªå½™ã‚µã‚¤ã‚ºã®å–å¾—
    tokenizer = AutoTokenizer.from_pretrained("facebook/wav2vec2-base-960h")
    temp_model = AutoModelForCTC.from_pretrained("facebook/wav2vec2-base-960h")
    vocab_size = temp_model.config.vocab_size
    
    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    model = VisionConditionedASR(vocab_size=vocab_size).to(device)
    
    # å±¤ã®å‡çµè¨­å®š
    freeze_layers(model, config)
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®ä½œæˆï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ç•°ãªã‚‹å­¦ç¿’ç‡ï¼‰
    optimizer = get_optimizer(model, config)
    
    # ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®è¨ˆç®—
    steps_per_epoch = len(train_dataloader) // config.gradient_accumulation_steps
    total_steps = steps_per_epoch * config.epochs
    
    # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®ä½œæˆ
    scheduler = get_scheduler(optimizer, config, total_steps)
    
    # æå¤±é–¢æ•°
    ctc_loss = nn.CTCLoss(blank=tokenizer.pad_token_id)
    
    print(f"\n[Training] Training configuration:")
    print(f"  âœ“ Total epochs: {config.epochs}")
    print(f"  âœ“ Steps per epoch: {steps_per_epoch}")
    print(f"  âœ“ Total steps: {total_steps}")
    print(f"  âœ“ Gradient clipping: max_norm={config.max_grad_norm}")
    
    # ========================================
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    # ========================================
    print("\n[Training] Starting training loop...")
    global_step = 0
    
    for epoch in range(config.epochs):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch+1}/{config.epochs}")
        print(f"{'='*60}")
        
        # å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º
        avg_train_loss, global_step = train_one_epoch(
            model=model,
            dataloader=train_dataloader,
            optimizer=optimizer,
            scheduler=scheduler,
            ctc_loss=ctc_loss,
            tokenizer=tokenizer,
            config=config,
            device=device,
            epoch=epoch,
            global_step=global_step
        )
        
        # è©•ä¾¡ãƒ•ã‚§ãƒ¼ã‚º
        avg_val_loss, avg_wer = validate(
            model=model,
            dataloader=val_dataloader,
            ctc_loss=ctc_loss,
            tokenizer=tokenizer,
            device=device
        )
        
        # çµæœã®ãƒ­ã‚®ãƒ³ã‚°
        if config.use_wandb:
            wandb.log({
                "train/epoch_loss": avg_train_loss,
                "val/loss": avg_val_loss,
                "val/wer": avg_wer,
                "epoch": epoch
            })
        
        # çµæœã®è¡¨ç¤º
        print(f"\n[Results] Epoch {epoch+1} Summary:")
        print(f"  Train Loss: {avg_train_loss:.4f}")
        print(f"  Val Loss:   {avg_val_loss:.4f}")
        print(f"  Val WER:    {avg_wer:.4f}")
        
        # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        save_model(model, epoch, config)
    
    # ========================================
    # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    # ========================================
    print("\n[Training] Training finished!")
    final_save_path = os.path.join(
        config.model_save_dir, 
        'vision_conditioned_asr_final.pth'
    )
    torch.save(model.state_dict(), final_save_path)
    print(f"[Save] Final model saved to {final_save_path}")
    
    # wandbã®çµ‚äº†
    if config.use_wandb:
        wandb.finish()
        print("[Wandb] Logging session finished")


if __name__ == "__main__":
    main()