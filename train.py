import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
import os
from datetime import datetime
from dataclasses import dataclass
from typing import Optional, List
import numpy as np

from model import VisionConditionedASR
from dataloader import create_dataloader


@dataclass
class TrainingConfig:
    """å­¦ç¿’è¨­å®š"""
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
    train_json: str = "../../Datasets/SpokenCOCO/SpokenCOCO_train.json"
    val_json: str = "../../Datasets/SpokenCOCO/SpokenCOCO_val.json"
    audio_dir: str = "../../Datasets/SpokenCOCO"
    image_dir: str = "../../Datasets/stair_captions/images"
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    vocab_size: Optional[int] = None  # Noneã§è‡ªå‹•å–å¾—
    hidden_dim: int = 256
    num_heads: int = 2
    
    # å­¦ç¿’è¨­å®š
    batch_size: int = 8
    num_epochs: int = 10
    learning_rate: float = 1e-4
    weight_decay: float = 1e-5
    gradient_clip: float = 1.0
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼è¨­å®š
    num_workers: int = 4
    max_audio_length: float = 10.0  # ç§’
    validate_files: bool = True
    
    # å±¤å‡çµè¨­å®š
    freeze_audio_encoder: bool = True
    freeze_vision_encoder: bool = True
    freeze_cross_attention: bool = False
    
    # å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
    warmup_steps: int = 1000
    
    # ä¿å­˜è¨­å®š
    checkpoint_dir: str = "../checkpoints"
    save_epoch: int = 1  # ã‚¨ãƒãƒƒã‚¯ã”ã¨
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device: str = "cuda:1"  # "cuda:0", "cuda:1", "cpu"
    
    # ãƒ­ã‚°è¨­å®š
    log_step: int = 100  # ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨
    validate_epoch: int = 1  # ã‚¨ãƒãƒƒã‚¯ã”ã¨


def freeze_layers(model: VisionConditionedASR, config: TrainingConfig):
    """
    æŒ‡å®šã•ã‚ŒãŸå±¤ã‚’å‡çµ
    
    Args:
        model: VisionConditionedASRãƒ¢ãƒ‡ãƒ«
        config: å­¦ç¿’è¨­å®š
    """
    print("\n" + "="*60)
    print("Layer Freeze Configuration")
    print("="*60)
    
    # Audio Encoderã®å‡çµ
    if config.freeze_audio_encoder:
        for param in model.audio_encoder.model.parameters():
            param.requires_grad = False
        print("âœ“ Audio Encoder (Wav2Vec2):    FROZEN")
    else:
        print("âœ“ Audio Encoder (Wav2Vec2):    Trainable")
    
    # Vision Encoderã®å‡çµ
    if config.freeze_vision_encoder:
        for param in model.vision_encoder.model.vision_model.parameters():
            param.requires_grad = False
        print("âœ“ Vision Encoder (CLIP):       FROZEN")
    else:
        print("âœ“ Vision Encoder (CLIP):       Trainable")
    
    # Cross Attentionã®å‡çµ
    if config.freeze_cross_attention:
        for param in model.cross_attention.parameters():
            param.requires_grad = False
        print("âœ“ Cross Attention:             FROZEN")
    else:
        print("âœ“ Cross Attention:             Trainable")
    
    # Classifierã¯å¸¸ã«å­¦ç¿’å¯èƒ½
    print("âœ“ Classifier (Linear):         Trainable (always)")
    
    # å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¡¨ç¤º
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    frozen_params = total_params - trainable_params
    
    print(f"\n{'='*60}")
    print("Parameter Statistics:")
    print(f"{'='*60}")
    print(f"Trainable parameters:  {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
    print(f"Frozen parameters:     {frozen_params:,} ({100 * frozen_params / total_params:.2f}%)")
    print(f"Total parameters:      {total_params:,}")
    print(f"{'='*60}\n")


def decode_predictions(
    predicted_ids: torch.Tensor, 
    tokenizer,
    blank_token_id: int = 0
) -> List[str]:
    """
    CTCã®äºˆæ¸¬çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆã«ãƒ‡ã‚³ãƒ¼ãƒ‰
    
    Args:
        predicted_ids: äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ID [batch, seq_len]
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        blank_token_id: CTCã®blankãƒˆãƒ¼ã‚¯ãƒ³IDï¼ˆé€šå¸¸ã¯0ï¼‰
        
    Returns:
        ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    decoded_texts = []
    
    for pred_ids_seq in predicted_ids:
        # CTCãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼š
        # 1. blank tokenã‚’é™¤å»
        # 2. é€£ç¶šã™ã‚‹åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ã‚’1ã¤ã«çµ±åˆï¼ˆcollapseï¼‰
        pred_tokens = []
        prev_token = None
        
        for token_id in pred_ids_seq.tolist():
            # blank tokenã¯ã‚¹ã‚­ãƒƒãƒ—
            if token_id == blank_token_id:
                prev_token = None  # blankãŒå‡ºãŸã‚‰prev_tokenã‚’ãƒªã‚»ãƒƒãƒˆ
                continue
            
            # é€£ç¶šã™ã‚‹åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ã¯1ã¤ã ã‘ä¿æŒ
            if token_id != prev_token:
                pred_tokens.append(token_id)
            
            prev_token = token_id
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
        decoded_text = tokenizer.decode(pred_tokens, skip_special_tokens=True)
        decoded_texts.append(decoded_text)
    
    return decoded_texts


def compute_ctc_loss(
    logits: torch.Tensor,
    texts: List[str],
    tokenizer,
    wav_lengths: torch.Tensor
) -> torch.Tensor:
    """
    CTCæå¤±ã‚’è¨ˆç®—
    
    Args:
        logits: ãƒ¢ãƒ‡ãƒ«å‡ºåŠ› [B, T, vocab_size]
        texts: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ†ã‚­ã‚¹ãƒˆ [B]
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        wav_lengths: å„éŸ³å£°ã®é•·ã• [B]
    
    Returns:
        loss: CTCæå¤±
    """
    batch_size = logits.size(0)
    device = logits.device
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    # CTCã§ã¯ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ãŒå¿…è¦
    target_ids = []
    target_lengths = []
    
    for text in texts:
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›
        tokens = tokenizer.encode(text, add_special_tokens=False)
        target_ids.extend(tokens)
        target_lengths.append(len(tokens))
    
    # Tensorã«å¤‰æ›
    target_ids = torch.tensor(target_ids, dtype=torch.long, device=device)
    target_lengths = torch.tensor(target_lengths, dtype=torch.long, device=device)
    
    # å…¥åŠ›é•·ã‚’è¨ˆç®—ï¼ˆlogitsã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼‰
    # Wav2Vec2ã¯éŸ³å£°ã‚’ç´„50å€åœ§ç¸®ã™ã‚‹ãŸã‚ã€å®Ÿéš›ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã¯ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‹ã‚‰å–å¾—
    input_lengths = torch.full((batch_size,), logits.size(1), dtype=torch.long, device=device)
    
    # CTCLossã®è¨ˆç®—
    # logitsã‚’ [T, B, vocab_size] ã«è»¢ç½®
    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)
    log_probs = log_probs.transpose(0, 1)  # [T, B, vocab_size]
    
    ctc_loss = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)
    
    try:
        loss = ctc_loss(log_probs, target_ids, input_lengths, target_lengths)
    except RuntimeError as e:
        print(f"\n[Warning] CTC Loss calculation error: {e}")
        print(f"  Input lengths: {input_lengths.tolist()}")
        print(f"  Target lengths: {target_lengths.tolist()}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¤§ããªæå¤±ã‚’è¿”ã™
        loss = torch.tensor(1e6, device=device, requires_grad=True)
    
    return loss


def train_one_epoch(
    model: VisionConditionedASR,
    dataloader: DataLoader,
    optimizer: torch.optim.Optimizer,
    tokenizer,
    device: torch.device,
    epoch: int,
    config: TrainingConfig
):
    """
    1ã‚¨ãƒãƒƒã‚¯åˆ†ã®å­¦ç¿’
    
    Args:
        model: ãƒ¢ãƒ‡ãƒ«
        dataloader: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        optimizer: ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        device: ãƒ‡ãƒã‚¤ã‚¹
        epoch: ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯ç•ªå·
        config: å­¦ç¿’è¨­å®š
    
    Returns:
        avg_loss: å¹³å‡æå¤±
    """
    model.train()
    total_loss = 0.0
    num_batches = len(dataloader)
    
    print(f"\n{'='*60}")
    print(f"Epoch {epoch+1}/{config.num_epochs} - Training")
    print(f"{'='*60}")
    
    for batch_idx, batch in enumerate(dataloader):
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•ï¼ˆwav_lengthsã®ã¿ï¼‰
            wav_lengths = batch["wav_lengths"].to(device)
            
            # Forward pass
            logits = model(batch)  # [B, T, vocab_size]
            
            # NaN/Infãƒã‚§ãƒƒã‚¯
            if torch.isnan(logits).any() or torch.isinf(logits).any():
                print(f"\nğŸš¨ CRITICAL: Logits contain NaN or Inf at batch {batch_idx}!")
                print(f"  Skipping this batch...")
                continue
            
            # CTCæå¤±ã®è¨ˆç®—
            loss = compute_ctc_loss(logits, batch["text"], tokenizer, wav_lengths)
            
            # NaN/Infãƒã‚§ãƒƒã‚¯
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"\nğŸš¨ CRITICAL: Loss is NaN or Inf at batch {batch_idx}!")
                print(f"  Skipping this batch...")
                continue
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)
            
            optimizer.step()
            
            # æå¤±ã®ç´¯ç©
            total_loss += loss.item()
            
            # ãƒ­ã‚°å‡ºåŠ›
            if (batch_idx + 1) % config.log_step == 0 or (batch_idx + 1) == num_batches:
                avg_loss = total_loss / (batch_idx + 1)
                print(f"  [{epoch+1}][{batch_idx+1}/{num_batches}] "
                      f"Loss: {loss.item():.4f} | Avg Loss: {avg_loss:.4f}")
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            if batch_idx % 50 == 0 and device.type == 'cuda':
                torch.cuda.empty_cache()
        
        except Exception as e:
            print(f"\n[Error] Exception at batch {batch_idx}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
    
    print(f"\n{'='*60}")
    print(f"Epoch {epoch+1} Training Summary:")
    print(f"  Average Loss: {avg_loss:.4f}")
    print(f"{'='*60}\n")
    
    return avg_loss


def validate(
    model: VisionConditionedASR,
    dataloader: DataLoader,
    tokenizer,
    device: torch.device,
    epoch: int,
    config: TrainingConfig,
    num_examples: int = 3
):
    """
    æ¤œè¨¼
    
    Args:
        model: ãƒ¢ãƒ‡ãƒ«
        dataloader: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        device: ãƒ‡ãƒã‚¤ã‚¹
        epoch: ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯ç•ªå·
        config: å­¦ç¿’è¨­å®š
        num_examples: è¡¨ç¤ºã™ã‚‹äºˆæ¸¬ä¾‹ã®æ•°
    
    Returns:
        avg_loss: å¹³å‡æå¤±
    """
    model.eval()
    total_loss = 0.0
    num_batches = 0
    all_predictions = []
    all_references = []
    
    print(f"\n{'='*60}")
    print(f"Epoch {epoch+1}/{config.num_epochs} - Validation")
    print(f"{'='*60}")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            try:
                # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
                wav_lengths = batch["wav_lengths"].to(device)
                
                # Forward pass
                logits = model(batch)
                
                # æå¤±è¨ˆç®—
                loss = compute_ctc_loss(logits, batch["text"], tokenizer, wav_lengths)
                
                if not (torch.isnan(loss) or torch.isinf(loss)):
                    total_loss += loss.item()
                    num_batches += 1
                
                # äºˆæ¸¬ã®ãƒ‡ã‚³ãƒ¼ãƒ‰
                predicted_ids = torch.argmax(logits, dim=-1)
                pred_texts = decode_predictions(predicted_ids, tokenizer, blank_token_id=0)
                
                # çµæœã®ä¿å­˜
                all_predictions.extend(pred_texts)
                all_references.extend(batch["text"])
                
            except Exception as e:
                print(f"\n[Error] Exception at validation batch {batch_idx}: {e}")
                continue
    
    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
    
    # äºˆæ¸¬ä¾‹ã‚’è¡¨ç¤º
    print(f"\n{'='*60}")
    print("Prediction Examples:")
    print(f"{'='*60}")
    
    for i in range(min(num_examples, len(all_predictions))):
        print(f"\nExample {i+1}:")
        print(f"  Reference:  {all_references[i][:80]}")
        print(f"  Prediction: {all_predictions[i][:80]}")
    
    print(f"\n{'='*60}")
    print(f"Epoch {epoch+1} Validation Summary:")
    print(f"  Average Loss: {avg_loss:.4f}")
    print(f"  Total samples: {len(all_predictions)}")
    print(f"{'='*60}\n")
    
    return avg_loss


def save_checkpoint(
    model: VisionConditionedASR,
    optimizer: torch.optim.Optimizer,
    epoch: int,
    train_loss: float,
    val_loss: float,
    config: TrainingConfig
):
    """
    ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜
    
    Args:
        model: ãƒ¢ãƒ‡ãƒ«
        optimizer: ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        epoch: ã‚¨ãƒãƒƒã‚¯ç•ªå·
        train_loss: è¨“ç·´æå¤±
        val_loss: æ¤œè¨¼æå¤±
        config: å­¦ç¿’è¨­å®š
    """
    os.makedirs(config.checkpoint_dir, exist_ok=True)
    
    checkpoint_path = os.path.join(
        config.checkpoint_dir,
        f"checkpoint_epoch_{epoch+1}.pt"
    )
    
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
        'config': config
    }, checkpoint_path)
    
    print(f"[Checkpoint] Saved to {checkpoint_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å­¦ç¿’é–¢æ•°"""
    # è¨­å®šã®åˆæœŸåŒ–
    config = TrainingConfig()
    
    # ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
    device = torch.device(config.device if torch.cuda.is_available() else "cpu")
    print(f"\n{'='*60}")
    print("Training Configuration")
    print(f"{'='*60}")
    print(f"Device: {device}")
    print(f"Batch size: {config.batch_size}")
    print(f"Learning rate: {config.learning_rate}")
    print(f"Num epochs: {config.num_epochs}")
    print(f"{'='*60}\n")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
    tokenizer = AutoTokenizer.from_pretrained("facebook/wav2vec2-base-960h")
    
    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    print("[Setup] Initializing model...")
    model = VisionConditionedASR(
        vocab_size=config.vocab_size,
        hidden_dim=config.hidden_dim,
        num_heads=config.num_heads,
        device=device
    ).to(device)
    
    # å±¤ã®å‡çµ
    freeze_layers(model, config)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ
    print("[Setup] Creating dataloaders...")
    train_loader = create_dataloader(
        json_path=config.train_json,
        audio_dir=config.audio_dir,
        image_dir=config.image_dir,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        max_audio_length=config.max_audio_length,
        validate_files=config.validate_files
    )
    
    val_loader = create_dataloader(
        json_path=config.val_json,
        audio_dir=config.audio_dir,
        image_dir=config.image_dir,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        max_audio_length=config.max_audio_length,
        validate_files=config.validate_files
    )
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®è¨­å®š
    optimizer = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    print("\n" + "="*60)
    print("Starting Training")
    print("="*60 + "\n")
    
    best_val_loss = float('inf')
    
    for epoch in range(config.num_epochs):
        # è¨“ç·´
        train_loss = train_one_epoch(
            model, train_loader, optimizer, tokenizer, device, epoch, config
        )
        
        # æ¤œè¨¼
        if (epoch + 1) % config.validate_epoch == 0:
            val_loss = validate(
                model, val_loader, tokenizer, device, epoch, config
            )
            
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                print(f"\nâœ¨ New best validation loss: {best_val_loss:.4f}")
                save_checkpoint(
                    model, optimizer, epoch, train_loss, val_loss, config
                )
        
        # å®šæœŸçš„ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        if (epoch + 1) % config.save_epoch == 0:
            save_checkpoint(
                model, optimizer, epoch, train_loss, 
                val_loss if 'val_loss' in locals() else 0.0, config
            )
    
    print("\n" + "="*60)
    print("Training Completed!")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()