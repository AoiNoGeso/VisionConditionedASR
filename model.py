from transformers import AutoTokenizer, AutoProcessor, AutoModelForCTC, CLIPProcessor, CLIPModel
import torch
import torch.nn as nn
from PIL import Image
import torchaudio
import numpy as np
import os

class AudioEncoder(nn.Module):
    """
    Wav2Vec2ã‚’ä½¿ç”¨ã—ãŸéŸ³å£°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
    
    å…¥åŠ›:
        data: dict with keys:
            - "wav": List[np.ndarray] - å„è¦ç´ ã¯(T,)ã®1æ¬¡å…ƒéŸ³å£°æ³¢å½¢
    
    å‡ºåŠ›:
        audio_features: Tensor[B, seq_len, 768] - éŸ³å£°ç‰¹å¾´é‡
    """
    
    def __init__(self, model_name="facebook/wav2vec2-base-960h", device=None):
        super().__init__()
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, force_download=True)
        self.processor = AutoProcessor.from_pretrained(model_name, force_download=True)
        
        # AutoModelForCTCã‚’ä½¿ç”¨
        self.model = AutoModelForCTC.from_pretrained(model_name, force_download=True)

        # ğŸ’¡ä¿®æ­£: masked_spec_embedã‚’ç„¡æ¡ä»¶ã§å†åˆæœŸåŒ–
        if hasattr(self.model, 'wav2vec2') and hasattr(self.model.wav2vec2, 'masked_spec_embed'):
            if self.model.wav2vec2.masked_spec_embed is not None:
                print("[AudioEncoder] Re-initializing masked_spec_embed to avoid NaN...")
                
                # å°ã•ã„ç¯„å›²ã®ä¸€æ§˜åˆ†å¸ƒã§åˆæœŸåŒ–
                nn.init.uniform_(
                    self.model.wav2vec2.masked_spec_embed.data,
                    a=-0.01,
                    b=0.01
                )
                
                # å†åˆæœŸåŒ–å¾Œã®ç¢ºèª
                if torch.isnan(self.model.wav2vec2.masked_spec_embed).any():
                    raise RuntimeError("ğŸš¨ CRITICAL: Failed to initialize masked_spec_embed!")
                
                print(f"[AudioEncoder] âœ“ masked_spec_embed initialized successfully")
                print(f"  Range: [{self.model.wav2vec2.masked_spec_embed.min().item():.6f}, "
                      f"{self.model.wav2vec2.masked_spec_embed.max().item():.6f}]")
        
        self.vocab_size = self.model.config.vocab_size
        print(f"[AudioEncoder] Vocab size: {self.vocab_size}")
        
        # ãƒ‡ãƒã‚¤ã‚¹ã®ä¿å­˜
        self._device = device
        
    def forward(self, data):
        """
        Args:
            data: dict with key "wav" (List[np.ndarray])
        
        Returns:
            audio_features: Tensor[B, seq_len, 768]
        """
        wav = data["wav"]
        
        # å…¥åŠ›ãƒã‚§ãƒƒã‚¯
        if not wav or len(wav) == 0:
            raise ValueError("Empty audio input received")
        
        for i, w in enumerate(wav):
            if len(w) == 0:
                raise ValueError(f"Audio sample {i} has zero length")
        
        # ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—
        if self._device is not None:
            device = self._device
        else:
            device = next(self.model.parameters()).device

        
        try:
            # ğŸ’¡ä¿®æ­£: éŸ³å£°ã®å‰å‡¦ç†ï¼ˆattention_maskã‚’å–å¾—ï¼‰
            processed = self.processor(
                wav,
                sampling_rate=16000, 
                return_tensors="pt", 
                padding=True
            )
            
            input_values = processed.input_values.to(device)
            attention_mask = processed.attention_mask.to(device) if hasattr(processed, 'attention_mask') else None
            
            # ğŸ’¡è¿½åŠ : å…¥åŠ›å€¤ã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
            if torch.isnan(input_values).any() or torch.isinf(input_values).any():
                raise RuntimeError("Input values contain NaN or Inf after processing")
            
            # ğŸ’¡è¿½åŠ : å…¥åŠ›å€¤ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯ï¼ˆç•°å¸¸ãªå€¤ã‚’ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼‰
            input_values = torch.clamp(input_values, min=-10.0, max=10.0)
            
        except Exception as e:
            print(f"Error in audio processing: {e}")
            print(f"  Input types: {[type(w) for w in wav]}")
            print(f"  Input shapes: {[w.shape if hasattr(w, 'shape') else len(w) for w in wav]}")
            raise
        
        # ğŸ’¡ä¿®æ­£: attention_maskã‚’æ¸¡ã—ã¦ç‰¹å¾´æŠ½å‡º
        audio_outputs = self.model.wav2vec2(
            input_values,
            attention_mask=attention_mask,  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ä½ç½®ã‚’æ˜ç¤º
            output_hidden_states=False,
            return_dict=True
        )
        audio_features = audio_outputs.last_hidden_state  # [B, seq_len, 768]
        
        # NaN/Infãƒã‚§ãƒƒã‚¯
        if torch.isnan(audio_features).any():
            print(f"\nğŸš¨ CRITICAL: NaN detected in audio features!")
            print(f"  Input values range: [{input_values.min():.4f}, {input_values.max():.4f}]")
            print(f"  Input values shape: {input_values.shape}")
            print(f"  Attention mask: {attention_mask}")
            raise RuntimeError("Audio features contain NaN values")
        if torch.isinf(audio_features).any():
            raise RuntimeError("Audio features contain Inf values")
        
        return audio_features


class VisionEncoder(nn.Module):
    """
    CLIPã‚’ä½¿ç”¨ã—ãŸç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
    
    å…¥åŠ›:
        data: dict with keys:
            - "image": List[PIL.Image.Image] - RGBç”»åƒã®ãƒªã‚¹ãƒˆ
    
    å‡ºåŠ›:
        image_features: Tensor[B, 512] - ç”»åƒç‰¹å¾´é‡ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«è¡¨ç¾ï¼‰
    
    Note:
        - ç”»åƒã¯è‡ªå‹•çš„ã«CLIPã®å…¥åŠ›ã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚ºã•ã‚Œã‚‹
        - äº‹å‰å­¦ç¿’æ¸ˆã¿CLIP ViT-B/16ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
    """
    
    def __init__(self, model_name="openai/clip-vit-base-patch16", device=None):
        super().__init__()
        self.model_name = model_name
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.model = CLIPModel.from_pretrained(model_name)
        
        # ãƒ‡ãƒã‚¤ã‚¹ã®ä¿å­˜
        self._device = device
        
    def forward(self, data):
        """
        Args:
            data: dict with key "image" (List[PIL.Image])
        
        Returns:
            image_features: Tensor[B, 512]
        """
        images = data["image"]
        
        # å…¥åŠ›ãƒã‚§ãƒƒã‚¯
        if not images or len(images) == 0:
            raise ValueError("Empty image input received")
        
        for i, img in enumerate(images):
            if not isinstance(img, Image.Image):
                raise TypeError(f"Image {i} is not a PIL.Image object: {type(img)}")
            if img.size[0] == 0 or img.size[1] == 0:
                raise ValueError(f"Image {i} has invalid size: {img.size}")
        
        # ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—
        if self._device is not None:
            device = self._device
        else:
            device = next(self.model.parameters()).device
        
        try:
            # ç”»åƒã®å‰å‡¦ç†
            inputs = self.processor(images=images, return_tensors="pt")
            
            # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                      for k, v in inputs.items()}
        except Exception as e:
            print(f"Error in image processing: {e}")
            print(f"  Input types: {[type(img) for img in images]}")
            print(f"  Input sizes: {[img.size for img in images]}")
            raise
        
        # ç‰¹å¾´æŠ½å‡º
        image_features = self.model.get_image_features(**inputs)  # [B, 512]
        
        # NaN/Infãƒã‚§ãƒƒã‚¯
        if torch.isnan(image_features).any():
            raise RuntimeError("Image features contain NaN values")
        if torch.isinf(image_features).any():
            raise RuntimeError("Image features contain Inf values")
        
        return image_features


class CrossAttention(nn.Module):
    """
    éŸ³å£°ã¨ç”»åƒç‰¹å¾´ã‚’çµ±åˆã™ã‚‹ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤
    
    Args:
        audio_dim: éŸ³å£°ç‰¹å¾´ã®æ¬¡å…ƒæ•°
        vision_dim: ç”»åƒç‰¹å¾´ã®æ¬¡å…ƒæ•°
        hidden_dim: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã®éš ã‚Œæ¬¡å…ƒæ•°
        num_heads: ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ãƒ˜ãƒƒãƒ‰æ•°
    """
    
    def __init__(self, audio_dim=768, vision_dim=512, hidden_dim=256, num_heads=2):
        super().__init__()
        self.audio_dim = audio_dim
        self.vision_dim = vision_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        
        # éŸ³å£°ç‰¹å¾´ã®å°„å½±
        self.audio_query = nn.Linear(audio_dim, hidden_dim)
        self.audio_key = nn.Linear(audio_dim, hidden_dim)
        self.audio_value = nn.Linear(audio_dim, hidden_dim)
        
        # ç”»åƒç‰¹å¾´ã®å°„å½±
        self.vision_key = nn.Linear(vision_dim, hidden_dim)
        self.vision_value = nn.Linear(vision_dim, hidden_dim)
        
        # ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
        self.multihead_attn = nn.MultiheadAttention(
            embed_dim=hidden_dim, 
            num_heads=num_heads, 
            batch_first=True,
            dropout=0.1
        )
        
        # å‡ºåŠ›å±¤
        self.output_proj = nn.Linear(hidden_dim, audio_dim)
        self.layer_norm = nn.LayerNorm(audio_dim)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, audio_features, vision_features):
        """
        Args:
            audio_features: [batch, seq_len, audio_dim] 
            vision_features: [batch, vision_dim]
        
        Returns:
            enhanced_audio_features: [batch, seq_len, audio_dim]
        """
        batch_size, seq_len, audio_dim = audio_features.size()
        vision_features = vision_features.unsqueeze(1)  # [B, 1, vision_dim]
        
        # ç‰¹å¾´ã®å°„å½±
        audio_q = self.audio_query(audio_features)  # [B, seq_len, hidden_dim]
        audio_k = self.audio_key(audio_features)    # [B, seq_len, hidden_dim]
        audio_v = self.audio_value(audio_features)  # [B, seq_len, hidden_dim]
        
        vision_k = self.vision_key(vision_features)   # [B, 1, hidden_dim]
        vision_v = self.vision_value(vision_features) # [B, 1, hidden_dim]
        
        # ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®key/valueã®çµåˆ
        keys = torch.cat([audio_k, vision_k], dim=1)      # [B, seq_len+1, hidden_dim]
        values = torch.cat([audio_v, vision_v], dim=1)    # [B, seq_len+1, hidden_dim]
        
        # ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆattention weightsã¯ä¿å­˜ã—ãªã„ï¼‰
        attn_output, _ = self.multihead_attn(
            query=audio_q,
            key=keys,
            value=values,
            need_weights=False  # ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
        )
        
        # å‡ºåŠ›å°„å½±ã¨æ®‹å·®æ¥ç¶š
        projected_output = self.output_proj(attn_output)
        output = self.layer_norm(audio_features + self.dropout(projected_output))
        
        return output


class VisionConditionedASR(nn.Module):
    """
    è¦–è¦šæƒ…å ±ã§æ¡ä»¶ä»˜ã‘ã•ã‚ŒãŸéŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«
    
    å…¥åŠ›:
        data: dict with keys:
            - "wav": List[np.ndarray] - éŸ³å£°æ³¢å½¢ã®ãƒªã‚¹ãƒˆ
            - "image": List[PIL.Image.Image] - ç”»åƒã®ãƒªã‚¹ãƒˆ
    
    å‡ºåŠ›:
        logits: Tensor[B, seq_len, vocab_size] - CTCç”¨ã®ãƒ­ã‚¸ãƒƒãƒˆ
    
    ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:
        1. AudioEncoder: éŸ³å£° -> [B, seq_len, 768]
        2. VisionEncoder: ç”»åƒ -> [B, 512]
        3. CrossAttention: éŸ³å£°ã¨ç”»åƒã®çµ±åˆ
        4. Classifier: [B, seq_len, vocab_size]ã¸å°„å½±
    
    Args:
        vocab_size: å‡ºåŠ›èªå½™ã‚µã‚¤ã‚ºï¼ˆNoneã§è‡ªå‹•å–å¾—ï¼‰
        hidden_dim: Cross-Attentionã®éš ã‚Œå±¤æ¬¡å…ƒ
        num_heads: ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ãƒ˜ãƒƒãƒ‰æ•°
        device: å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹ï¼ˆNoneã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•å–å¾—ï¼‰
    """
    
    def __init__(self, vocab_size=None, hidden_dim=256, num_heads=2, device=None):
        super().__init__()
        self._device = device
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®åˆæœŸåŒ–
        self.audio_encoder = AudioEncoder(device=device)
        self.vision_encoder = VisionEncoder(device=device)
        
        # èªå½™ã‚µã‚¤ã‚ºã®è‡ªå‹•å–å¾—
        if vocab_size is None:
            vocab_size = self.audio_encoder.vocab_size
            print(f"[Model] Auto-detected vocab_size: {vocab_size}")
        
        self.vocab_size = vocab_size
        
        # ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤
        self.cross_attention = CrossAttention(
            audio_dim=768,
            vision_dim=512,
            hidden_dim=hidden_dim,
            num_heads=num_heads
        )
        
        # åˆ†é¡å™¨
        self.classifier = nn.Linear(768, vocab_size)
        
    def forward(self, data):
        """
        Args:
            data: dict with keys "wav" and "image"
        
        Returns:
            logits: Tensor[B, seq_len, vocab_size]
        """
        # éŸ³å£°ç‰¹å¾´æŠ½å‡º: [B, seq_len, 768]
        audio_features = self.audio_encoder(data)
        
        # ç”»åƒç‰¹å¾´æŠ½å‡º: [B, 512]
        vision_features = self.vision_encoder(data)
        
        # ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³: [B, seq_len, 768]
        enhanced_audio = self.cross_attention(audio_features, vision_features)
        
        # ä¸­é–“ç‰¹å¾´ã‚’å‰Šé™¤ï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰
        del audio_features, vision_features
        
        # åˆ†é¡: [B, seq_len, vocab_size]
        output_logits = self.classifier(enhanced_audio)
        
        return output_logits


def demo():
    """ãƒ‡ãƒ¢å®Ÿè¡Œé–¢æ•°"""
    print("="*60)
    print("Vision-Conditioned ASR Demo")
    print("="*60)
    
    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    print("\n[1/4] Initializing model...")
    avsr = VisionConditionedASR()
    avsr.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
    wav_path = "../../Datasets/SpokenCOCO/wavs/val/0/m071506418gb9vo0w5xq3-3LUY3GC63Z0R9PYEETJGN5HO4UEP7B_325114_629297.wav"
    img_path = "../../Datasets/stair_captions/images/val2014/COCO_val2014_000000325114.jpg"
    
    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\n[2/4] Loading audio data...")
    wav, sr = torchaudio.load(wav_path)
    if sr != 16000:
        wav = torchaudio.transforms.Resample(sr, 16000)(wav)
    if wav.shape[0] > 1:
        wav = wav.mean(dim=0, keepdim=True)
    wav = wav.squeeze(0)
    print(f"  Audio shape: {wav.shape}, Sample rate: 16000Hz")
    
    # ç”»åƒãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\n[3/4] Loading image data...")
    image = Image.open(img_path).convert('RGB')
    print(f"  Image size: {image.size}, Mode: {image.mode}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆãƒãƒƒãƒå½¢å¼ï¼‰
    sample = {
        "wav": [wav.numpy()],  # Listå½¢å¼
        "image": [image],        # Listå½¢å¼
        "text": "A URINAL IN A PUBLIC RESTROOM NEAR A WOODEN TABLE"
    }
    
    # æ¨è«–å®Ÿè¡Œ
    print("\n[4/4] Running inference...")
    with torch.no_grad():
        # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡ºåŠ›ç¢ºèª
        audio_outputs = avsr.audio_encoder(data=sample)
        vision_outputs = avsr.vision_encoder(data=sample)
        asr_outputs = avsr(data=sample)
    
    print(f"\n{'='*60}")
    print("Output Shapes:")
    print(f"{'='*60}")
    print(f"  Audio features:  {audio_outputs.shape}")
    print(f"  Vision features: {vision_outputs.shape}")
    print(f"  ASR logits:      {asr_outputs.shape}")
    
    # ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†
    print(f"\n{'='*60}")
    print("Decoding Results:")
    print(f"{'='*60}")
    
    tokenizer = avsr.audio_encoder.tokenizer
    
    # logitsã‹ã‚‰äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å–å¾—
    predicted_ids = torch.argmax(asr_outputs, dim=-1)  # [B, seq_len]
    
    # CTC blank tokenã®IDï¼ˆé€šå¸¸ã¯0ï¼‰
    blank_token_id = 0
    
    for i, pred_ids in enumerate(predicted_ids):
        pred_ids = pred_ids.cpu().numpy()
        
        # CTCãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        pred_tokens = []
        prev_token = None
        
        for token_id in pred_ids:
            # blank tokenã‚’ã‚¹ã‚­ãƒƒãƒ—
            if token_id == blank_token_id:
                prev_token = None
                continue
            
            # é€£ç¶šã™ã‚‹åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ã¯1ã¤ã ã‘ä¿æŒ
            if token_id != prev_token:
                pred_tokens.append(token_id)
            
            prev_token = token_id
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        transcription = tokenizer.decode(pred_tokens, skip_special_tokens=True)
        
        print(f"\nSample {i}:")
        print(f"  Ground Truth: {sample['text']}")
        print(f"  Prediction:   {transcription}")
        print(f"  Note: Model is untrained, output is random")
    
    print(f"\n{'='*60}")
    print("Demo completed successfully!")
    print(f"{'='*60}\n")


def demo_batch():
    """ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹ãƒ‡ãƒ¢å®Ÿè¡Œé–¢æ•°ï¼ˆè¤‡æ•°ã®éŸ³å£°ã¨ç”»åƒã‚’åŒæ™‚ã«å‡¦ç†ï¼‰"""
    print("="*60)
    print("Vision-Conditioned ASR Batch Demo")
    print("="*60)
    
    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    print("\n[1/5] Initializing model...")
    avsr = VisionConditionedASR()
    avsr.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
    
    sample_files = [
        {
            "wav": "../../Datasets/SpokenCOCO/wavs/val/0/m071506418gb9vo0w5xq3-3LUY3GC63Z0R9PYEETJGN5HO4UEP7B_325114_629297.wav",
            "img": "../../Datasets/stair_captions/images/val2014/COCO_val2014_000000325114.jpg",
            "text": "A URINAL IN A PUBLIC RESTROOM NEAR A WOODEN TABLE"
        },
        # ğŸ’¡æ”¹å–„: å®Ÿéš›ã«ã¯ç•°ãªã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨
        # ä»¥ä¸‹ã¯2ã¤ç›®ã®ã‚µãƒ³ãƒ—ãƒ«ä¾‹ï¼ˆå®Ÿéš›ã®ãƒ‘ã‚¹ã«ç½®ãæ›ãˆã‚‹ï¼‰
        {
            "wav": "../../Datasets/SpokenCOCO/wavs/val/0/m1a5mox83rrx60-3V5Q80FXIXRDGZWLAJ5EEBXFON723D_297698_737627.wav",
            "img": "../../Datasets/stair_captions/images/val2014/COCO_val2014_000000297698.jpg",
            "text": "THE SKIER TAKES OFF DOWN THE STEEP HILL"
        }
    ]
    
    # ğŸ’¡è¿½åŠ : ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã¨éŸ³å£°/ç”»åƒã®ãƒ­ãƒ¼ãƒ‰
    wavs = []
    images = []
    texts = []
    
    print("\n[2/5] Loading samples...")
    for i, sample_file in enumerate(sample_files):
        try:
            # ğŸ’¡è¿½åŠ : ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            if not os.path.exists(sample_file["wav"]):
                print(f"  âš ï¸  Sample {i+1}: Audio file not found, skipping...")
                continue
            if not os.path.exists(sample_file["img"]):
                print(f"  âš ï¸  Sample {i+1}: Image file not found, skipping...")
                continue
            
            # éŸ³å£°èª­ã¿è¾¼ã¿
            wav, sr = torchaudio.load(sample_file["wav"])
            if sr != 16000:
                wav = torchaudio.transforms.Resample(sr, 16000)(wav)
            if wav.shape[0] > 1:
                wav = wav.mean(dim=0, keepdim=True)
            wav = wav.squeeze(0).numpy()
            
            # ç”»åƒèª­ã¿è¾¼ã¿
            image = Image.open(sample_file["img"]).convert('RGB')
            
            wavs.append(wav)
            images.append(image)
            texts.append(sample_file["text"])
            
            # ğŸ’¡è¿½åŠ : å„ã‚µãƒ³ãƒ—ãƒ«ã®æƒ…å ±ã‚’è¡¨ç¤º
            print(f"  âœ“ Sample {i+1}: Audio shape={wav.shape}, Image size={image.size}")
            
        except Exception as e:
            print(f"  âœ— Sample {i+1}: Error loading - {e}")
            continue
    
    # ğŸ’¡è¿½åŠ : ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ãŒ0ã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼
    if len(wavs) == 0:
        print("\nâŒ No valid samples loaded. Please check file paths.")
        return
    
    # ğŸ’¡è¿½åŠ : éŸ³å£°é•·ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    wav_lengths = [len(w) for w in wavs]
    print(f"\n[3/5] Audio length statistics:")
    print(f"  Min length: {min(wav_lengths):,} samples ({min(wav_lengths)/16000:.2f}s)")
    print(f"  Max length: {max(wav_lengths):,} samples ({max(wav_lengths)/16000:.2f}s)")
    print(f"  Mean length: {np.mean(wav_lengths):,.0f} samples ({np.mean(wav_lengths)/16000:.2f}s)")
    print(f"  â†’ Padding will be applied to max length")
    
    # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    batch_sample = {
        "wav": wavs,
        "image": images,
        "text": texts
    }
    
    # æ¨è«–å®Ÿè¡Œ
    print(f"\n[4/5] Running batch inference (Batch Size: {len(batch_sample['wav'])})...")
    try:
        with torch.no_grad():
            # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡ºåŠ›ç¢ºèª
            audio_outputs = avsr.audio_encoder(data=batch_sample)
            vision_outputs = avsr.vision_encoder(data=batch_sample)
            asr_outputs = avsr(data=batch_sample)
        
        # --- å‡ºåŠ›å½¢çŠ¶ã®ç¢ºèª ---
        print(f"\n{'='*60}")
        print("Batch Output Shapes:")
        print(f"{'='*60}")
        print(f"  Audio features:  {audio_outputs.shape}  # [B, seq_len, 768]")
        print(f"  Vision features: {vision_outputs.shape}  # [B, 512]")
        print(f"  ASR logits:      {asr_outputs.shape}    # [B, seq_len, vocab_size]")
        print(f"  Batch size (B):  {asr_outputs.shape[0]}")
        
        # ğŸ’¡è¿½åŠ : NaN/Infãƒã‚§ãƒƒã‚¯
        if torch.isnan(asr_outputs).any():
            print("\nâš ï¸  WARNING: NaN detected in ASR outputs!")
        if torch.isinf(asr_outputs).any():
            print("\nâš ï¸  WARNING: Inf detected in ASR outputs!")
        
    except Exception as e:
        print(f"\nâŒ Inference failed: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # --- ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç† ---
    print(f"\n[5/5] Decoding Batch Results...")
    print(f"{'='*60}")
    
    tokenizer = avsr.audio_encoder.tokenizer
    
    # logitsã‹ã‚‰äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å–å¾—
    predicted_ids = torch.argmax(asr_outputs, dim=-1)  # [B, seq_len]
    
    # CTC blank tokenã®IDï¼ˆé€šå¸¸ã¯0ï¼‰
    blank_token_id = 0
    
    for i, pred_ids in enumerate(predicted_ids):
        pred_ids = pred_ids.cpu().numpy()
        
        # CTCãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        pred_tokens = []
        prev_token = None
        
        for token_id in pred_ids:
            # blank tokenã‚’ã‚¹ã‚­ãƒƒãƒ—
            if token_id == blank_token_id:
                prev_token = None
                continue
            
            # é€£ç¶šã™ã‚‹åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ã¯1ã¤ã ã‘ä¿æŒ
            if token_id != prev_token:
                pred_tokens.append(token_id)
            
            prev_token = token_id
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        transcription = tokenizer.decode(pred_tokens, skip_special_tokens=True)
        
        print(f"\nSample {i+1} / {len(predicted_ids)}:")
        print(f"  Audio length: {wav_lengths[i]:,} samples ({wav_lengths[i]/16000:.2f}s)")
        print(f"  Ground Truth: {batch_sample['text'][i]}")
        print(f"  Prediction:   {transcription}")
        print(f"  Note: Model is untrained, output is random")
    
    print(f"\n{'='*60}")
    print("Batch Demo completed successfully!")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    # demo()         # å˜ä¸€ãƒ‡ãƒ¼ã‚¿ç‰ˆ
    demo_batch()    # ãƒãƒƒãƒå‡¦ç†ç‰ˆ

